{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle Component Analysis Alogirthm\n",
    "\n",
    "Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The number of distinct principal components is equal to the smaller of the number of original variables or the number of observations minus one. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset and library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "column = [' timedelta', ' n_tokens_title', ' n_tokens_content',\n",
    "       ' n_unique_tokens', ' n_non_stop_words', ' n_non_stop_unique_tokens',\n",
    "       ' num_hrefs', ' num_self_hrefs', ' num_imgs', ' num_videos',\n",
    "       ' average_token_length', ' num_keywords', ' data_channel_is_lifestyle',\n",
    "       ' data_channel_is_entertainment', ' data_channel_is_bus',\n",
    "       ' data_channel_is_socmed', ' data_channel_is_tech',\n",
    "       ' data_channel_is_world', ' kw_min_min', ' kw_max_min', ' kw_avg_min',\n",
    "       ' kw_min_max', ' kw_max_max', ' kw_avg_max', ' kw_min_avg',\n",
    "       ' kw_max_avg', ' kw_avg_avg', ' self_reference_min_shares',\n",
    "       ' self_reference_max_shares', ' self_reference_avg_sharess',\n",
    "       ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday',\n",
    "       ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday',\n",
    "       ' weekday_is_sunday', ' is_weekend', ' LDA_00', ' LDA_01', ' LDA_02',\n",
    "       ' LDA_03', ' LDA_04', ' global_subjectivity',\n",
    "       ' global_sentiment_polarity', ' global_rate_positive_words',\n",
    "       ' global_rate_negative_words', ' rate_positive_words',\n",
    "       ' rate_negative_words', ' avg_positive_polarity',\n",
    "       ' min_positive_polarity', ' max_positive_polarity',\n",
    "       ' avg_negative_polarity', ' min_negative_polarity',\n",
    "       ' max_negative_polarity', ' title_subjectivity',\n",
    "       ' title_sentiment_polarity', ' abs_title_subjectivity',\n",
    "       ' abs_title_sentiment_polarity', 'Share']\n",
    "data = pd.read_csv(\"Online_news_NN/train.csv\")\n",
    "data.columns = column\n",
    "#data1 = pd.read_csv(\"OnlineNewsPopularity1.csv\")\n",
    "#print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting target variable and remaining attributes for Principle Components Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([' timedelta', ' n_tokens_title', ' n_tokens_content',\n",
       "       ' n_unique_tokens', ' n_non_stop_words', ' n_non_stop_unique_tokens',\n",
       "       ' num_hrefs', ' num_self_hrefs', ' num_imgs', ' num_videos',\n",
       "       ' average_token_length', ' num_keywords', ' data_channel_is_lifestyle',\n",
       "       ' data_channel_is_entertainment', ' data_channel_is_bus',\n",
       "       ' data_channel_is_socmed', ' data_channel_is_tech',\n",
       "       ' data_channel_is_world', ' kw_min_min', ' kw_max_min', ' kw_avg_min',\n",
       "       ' kw_min_max', ' kw_max_max', ' kw_avg_max', ' kw_min_avg',\n",
       "       ' kw_max_avg', ' kw_avg_avg', ' self_reference_min_shares',\n",
       "       ' self_reference_max_shares', ' self_reference_avg_sharess',\n",
       "       ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday',\n",
       "       ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday',\n",
       "       ' weekday_is_sunday', ' is_weekend', ' LDA_00', ' LDA_01', ' LDA_02',\n",
       "       ' LDA_03', ' LDA_04', ' global_subjectivity',\n",
       "       ' global_sentiment_polarity', ' global_rate_positive_words',\n",
       "       ' global_rate_negative_words', ' rate_positive_words',\n",
       "       ' rate_negative_words', ' avg_positive_polarity',\n",
       "       ' min_positive_polarity', ' max_positive_polarity',\n",
       "       ' avg_negative_polarity', ' min_negative_polarity',\n",
       "       ' max_negative_polarity', ' title_subjectivity',\n",
       "       ' title_sentiment_polarity', ' abs_title_subjectivity',\n",
       "       ' abs_title_sentiment_polarity', 'Share'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = data.loc[1:,[' timedelta', ' n_tokens_title', ' n_tokens_content',\n",
    "       ' n_unique_tokens', ' n_non_stop_words', ' n_non_stop_unique_tokens',\n",
    "       ' num_hrefs', ' num_self_hrefs', ' num_imgs', ' num_videos',\n",
    "       ' average_token_length', ' num_keywords', ' data_channel_is_lifestyle',\n",
    "       ' data_channel_is_entertainment', ' data_channel_is_bus',\n",
    "       ' data_channel_is_socmed', ' data_channel_is_tech',\n",
    "       ' data_channel_is_world', ' kw_min_min', ' kw_max_min', ' kw_avg_min',\n",
    "       ' kw_min_max', ' kw_max_max', ' kw_avg_max', ' kw_min_avg',\n",
    "       ' kw_max_avg', ' kw_avg_avg', ' self_reference_min_shares',\n",
    "       ' self_reference_max_shares', ' self_reference_avg_sharess',\n",
    "       ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday',\n",
    "       ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday',\n",
    "       ' weekday_is_sunday', ' is_weekend', ' LDA_00', ' LDA_01', ' LDA_02',\n",
    "       ' LDA_03', ' LDA_04', ' global_subjectivity',\n",
    "       ' global_sentiment_polarity', ' global_rate_positive_words',\n",
    "       ' global_rate_negative_words', ' rate_positive_words',\n",
    "       ' rate_negative_words', ' avg_positive_polarity',\n",
    "       ' min_positive_polarity', ' max_positive_polarity',\n",
    "       ' avg_negative_polarity', ' min_negative_polarity',\n",
    "       ' max_negative_polarity', ' title_subjectivity',\n",
    "       ' title_sentiment_polarity', ' abs_title_subjectivity',\n",
    "       ' abs_title_sentiment_polarity']]\n",
    "y_df = data.loc[1:,'Share']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>731</td>\n",
       "      <td>9</td>\n",
       "      <td>211</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.495833</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>731</td>\n",
       "      <td>9</td>\n",
       "      <td>531</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385965</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>731</td>\n",
       "      <td>13</td>\n",
       "      <td>1072</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411127</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>731</td>\n",
       "      <td>10</td>\n",
       "      <td>370</td>\n",
       "      <td>0.559889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.698198</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350610</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.195000</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>731</td>\n",
       "      <td>8</td>\n",
       "      <td>960</td>\n",
       "      <td>0.418163</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.549834</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402039</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.224479</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    timedelta   n_tokens_title   n_tokens_content   n_unique_tokens  \\\n",
       "1         731                9                211          0.575130   \n",
       "2         731                9                531          0.503788   \n",
       "3         731               13               1072          0.415646   \n",
       "4         731               10                370          0.559889   \n",
       "5         731                8                960          0.418163   \n",
       "\n",
       "    n_non_stop_words   n_non_stop_unique_tokens   num_hrefs   num_self_hrefs  \\\n",
       "1                1.0                   0.663866           3                1   \n",
       "2                1.0                   0.665635           9                0   \n",
       "3                1.0                   0.540890          19               19   \n",
       "4                1.0                   0.698198           2                2   \n",
       "5                1.0                   0.549834          21               20   \n",
       "\n",
       "    num_imgs   num_videos              ...                \\\n",
       "1          1            0              ...                 \n",
       "2          1            0              ...                 \n",
       "3         20            0              ...                 \n",
       "4          0            0              ...                 \n",
       "5         20            0              ...                 \n",
       "\n",
       "    avg_positive_polarity   min_positive_polarity   max_positive_polarity  \\\n",
       "1                0.495833                0.100000                     1.0   \n",
       "2                0.385965                0.136364                     0.8   \n",
       "3                0.411127                0.033333                     1.0   \n",
       "4                0.350610                0.136364                     0.6   \n",
       "5                0.402039                0.100000                     1.0   \n",
       "\n",
       "    avg_negative_polarity   min_negative_polarity   max_negative_polarity  \\\n",
       "1               -0.466667                    -0.8               -0.133333   \n",
       "2               -0.369697                    -0.6               -0.166667   \n",
       "3               -0.220192                    -0.5               -0.050000   \n",
       "4               -0.195000                    -0.4               -0.100000   \n",
       "5               -0.224479                    -0.5               -0.050000   \n",
       "\n",
       "    title_subjectivity   title_sentiment_polarity   abs_title_subjectivity  \\\n",
       "1             0.000000                   0.000000                 0.500000   \n",
       "2             0.000000                   0.000000                 0.500000   \n",
       "3             0.454545                   0.136364                 0.045455   \n",
       "4             0.642857                   0.214286                 0.142857   \n",
       "5             0.000000                   0.000000                 0.500000   \n",
       "\n",
       "    abs_title_sentiment_polarity  \n",
       "1                       0.000000  \n",
       "2                       0.000000  \n",
       "3                       0.136364  \n",
       "4                       0.214286  \n",
       "5                       0.000000  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39642, 59)\n",
      "(1, 39642)\n"
     ]
    }
   ],
   "source": [
    "X = np.asmatrix(X_df)\n",
    "y = np.asmatrix(y_df)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "#X = X.sub(X.mean(axis=0), axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization\n",
    "\n",
    "Zero mean feature Normalization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetaure_norm(X):\n",
    "    mu = X.mean(axis=0)\n",
    "    stdv = X.std(axis = 0)\n",
    "\n",
    "    X_norm = (X - mu)/stdv\n",
    "    return X_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle Component Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X):\n",
    "    m,n = X.shape\n",
    "    \n",
    "    sigma = (1/m) * X.T * X\n",
    "    a, b = np.linalg.eig(sigma)\n",
    "    \n",
    "    sort = a.argsort()[::-1]\n",
    "    eigVal = a[sort]\n",
    "    eigVec = b[:,sort]\n",
    "    \n",
    "    return eigVal,eigVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduced_data(X, U, k):\n",
    "    U_reduce = U[:,:k]\n",
    "    Z = U_reduce.T * X.T\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction from compressed representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recover_data(Z,U,K):\n",
    "    X_rec = Z.T * U[:,:k].T\n",
    "    return X_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variance_cal(S,k,m):\n",
    "    total1 = 0\n",
    "    total2 = 0\n",
    "    for i in range(1,k):\n",
    "        total1 = total1 + S[i]\n",
    "    \n",
    "    for j in range(1,m):\n",
    "        total2 = total2 + S[j]\n",
    "        \n",
    "    variance = total1/total2\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions\n",
    "\n",
    "calling all the function to perform PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimum variance is achieved in  32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_norm = fetaure_norm(X)\n",
    "m,n = X_norm.shape\n",
    "a,b = pca(X_norm)\n",
    "\n",
    "for k in range(2,n):\n",
    "    Z = reduced_data(X_norm,b,k)\n",
    "    X_recov = recover_data(Z,b,k)\n",
    "    test1 = variance_cal(a,k,n)\n",
    "    if(test1>0.9):\n",
    "        print(\"Optimum variance is achieved in \", k)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
